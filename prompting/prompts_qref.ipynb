{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load credentials\n",
    "with open(\"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/secrets.yml\", 'r') as file:\n",
    "    credentials = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing LLMs (Llama via AWS Bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatBedrock with LLaMA 3.3 70B\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "    region=\"us-east-1\",\n",
    "    aws_access_key_id=credentials[\"bedrock\"][\"access_key\"],\n",
    "    aws_secret_access_key=credentials[\"bedrock\"][\"secret_key\"],\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatBedrock with LLaMA 3.2 3B\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "    region=\"us-east-1\",\n",
    "    aws_access_key_id=credentials[\"bedrock\"][\"access_key\"],\n",
    "    aws_secret_access_key=credentials[\"bedrock\"][\"secret_key\"],\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatBedrock with LLaMA 3.1 8B\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.meta.llama3-1-8b-instruct-v1:0\",\n",
    "    region=\"us-east-1\",\n",
    "    aws_access_key_id=credentials[\"bedrock\"][\"access_key\"],\n",
    "    aws_secret_access_key=credentials[\"bedrock\"][\"secret_key\"],\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Prompt Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/QREF/qref_session.json\"\n",
    "output_file = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/QREF/qref_session_llama_3.2_3b.json\"\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "def create_prompt(system_message, user_message):\n",
    "    return f\"{system_message}\\n\\n{user_message}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASELINE PROMPT (DNA+CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "system_message = \"\"\"You are a search quality rater evaluating the usefulness of clicked documents on a web page clicked by users in a session. You have been given the following session information:\n",
    "  a.  User: User ID\n",
    "  b.  Session ID: Unique ID for each session.\n",
    "  c.  Query ID: Unique ID for each query.\n",
    "  d.  Query: User submitted query.\n",
    "  e.  ID: Unique ID for each row.\n",
    "  f.  Clicked Documents: User clicked URL.\n",
    "  g.  Title: Title of the clicked documents.\n",
    "  h.  Content: Brief description of the clicked documents on the web page.\n",
    "  i.  Rank: Rank of clicked documents on the SERP labeled as 1(high)~10(low).\n",
    "  j.  Task Satisfaction: User‚Äôs satisfaction on overall task labeled as 0(low)~4(high).\n",
    "  k.  Query Satisfaction: User‚Äôs satisfaction on a search query labeled as 0(low)~4(high).\n",
    "  l.  CTR: The percentage of clicks a document (URL) receives out of total interactions within a specific task and query.\n",
    "  m.  Session Dwell Time (ms): Time spent on a specific task across all queries and clicked documents.\n",
    "  n.  Query Dwell Time (ms): Time spent on a specific query.\n",
    "\n",
    "\n",
    "You must analyze each user-session by considering all the information given above and for each clicked document (URL), provide a usefulness score on an integer scale of 0 to 3 with the following meanings:\n",
    "3 = Very Useful, very helpful for this query\n",
    "2 = Fairly Useful, fairly helpful for this query\n",
    "1 = Somewhat Useful, maybe partly helpful but might contain other irrelevant content\n",
    "0 = Not Useful at all, should never be shown for this query\n",
    "\n",
    "Important Instructions:\n",
    "Consider all the attributes above while deciding on a usefulness score. If certain attributes are unavailable, rely on the available ones to decide. Assign category 1 if the\n",
    "clicked document is somewhat useful to the task and query but not completely, category 2 if the clicked document presents something very important related to the task and\n",
    "query but also has some extra information, and category 3 if the clicked document only and entirely refers to the task and document. If none of the above satisfies give it category 0.\n",
    "\n",
    "You will be provided data in the form of:\n",
    "[\n",
    "    {\n",
    "        \"user_id\": \"<USER>\",\n",
    "        \"session_id\": \"<SESSION_ID>\",\n",
    "        \"session_dwell_time\": \"<SESSION_DWELL_TIME>\",\n",
    "        \"session_sat_score\": \"<SESSION_SAT_SCORE>\",\n",
    "        \"query_id\": \"<QUERY_ID>\",\n",
    "        \"query_string_en\": \"<QUERY_TEXT>\",\n",
    "        \"query_dwell_time\": \"<QUERY_DWELL_TIME>\",\n",
    "        \"query_sat_score\": \"<QUERY_SAT_SCORE>\",\n",
    "        \"id\": \"<ID>\",\n",
    "        \"url\": \"<URL>\",\n",
    "        \"title_en\": \"<TITLE>\",\n",
    "        \"content_en\": \"<CONTENT>\",\n",
    "        \"rank\": \"<RANK>\",\n",
    "        \"CTR\": \"<CLICK_THROUGH_RATE>\",\n",
    "      }\n",
    "]\n",
    "\n",
    "For each user <USER> and each task session <TASK_ID>, for each clicked document <URL>, split this problem into steps:\n",
    "a. Consider ALL the attributes and relative importance of each and decide on a final score <usefulness_i>. Final score must be an integer value only.\n",
    "b. Prioritise ALL user metrics like CTR, query dwell time and session dwell time as indicators of usefulness.\n",
    "c. Consider user's intent of the query and session ensuring they align with the title and content.\n",
    "d. Consider the rank of URLs relation to the user‚Äôs query.\n",
    "e. Consider user‚Äôs query satisfaction score and session satisfaction score, which reflect their satisfaction with the result.\n",
    "\n",
    "\n",
    "ONLY PROVIDE OUTPUT IN GIVEN FORMAT. Directly output the usefulness score for unique ID <ID> as an integer value in the following JSON format:\n",
    "{\\\"<id_1>\\\": <usefulness_1>, ..., \\\"<id_n>\\\": <usefulness_n>}\n",
    "\n",
    "GENERATE USEFULNESS SCORE IN CORRECT OUTPUT FORMAT FOR ALL UNIQUE ID. DO NOT PROVIDE ADDITIONAL TEXT, REASONING, EXAMPLE, OR CODE. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QREF BASELINE ROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create batched input\n",
    "def create_batched_input(batch):\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"\"\"\n",
    "        user: {row['user_id']}\n",
    "        session_id: {row['session_id']}\n",
    "        session_dwell_time: {row['session_dwell_time']}\n",
    "        session_sat_score: {row['session_sat_score']}\n",
    "        query_id: {row['query_id']}\n",
    "        query_string_en: {row['query_string_en']}\n",
    "        query_dwell_time: {row['query_dwell_time']}\n",
    "        query_sat_score: {row['query_sat_score']}\n",
    "        id: {row['id']}\n",
    "        url: {row['url']}\n",
    "        title_en: {row['title_en']}\n",
    "        content_en: {row['content_en']}\n",
    "        rank: {row['rank']}\n",
    "        CTR: {row['CTR']}\n",
    "        \"\"\"\n",
    "        for row in batch\n",
    "    ).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recover JSON from a truncated response\n",
    "def recover_json(truncated_response):\n",
    "    try:\n",
    "        truncated_response = truncated_response.strip('```json').strip('```').strip()\n",
    "        for i in range(len(truncated_response), 0, -1):\n",
    "            try:\n",
    "                return json.loads(truncated_response[:i])\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error recovering JSON: {e}\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a batch with retry and truncated response handling\n",
    "def process_batch_with_retry(batch, retries=3, backoff_factor=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            batched_user_message = create_batched_input(batch)\n",
    "            full_prompt = create_prompt(system_message, batched_user_message)\n",
    "            # Send the combined prompt to the LLM\n",
    "            response = llm.predict(full_prompt)  \n",
    "            print(f\"Raw LLM response:\\n{response}\") \n",
    "\n",
    "            cleaned_response = response.strip('```json').strip('```').strip() \n",
    "\n",
    "            if not cleaned_response.endswith(\"}\"):\n",
    "                print(\"Warning: Truncated response detected.\")\n",
    "                recovered_response = recover_json(cleaned_response)\n",
    "                if recovered_response:\n",
    "                    print(\"Recovered partial JSON response successfully.\")\n",
    "                    return recovered_response\n",
    "                print(\"Unable to recover truncated response. Retrying with smaller batch size.\")\n",
    "                smaller_batches = [batch[j:j+len(batch)//2] for j in range(0, len(batch), len(batch)//2)]\n",
    "                for small_batch in smaller_batches:\n",
    "                    process_batch_with_retry(small_batch)\n",
    "                return {}\n",
    "\n",
    "            return json.loads(cleaned_response)\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"Rate limit\" in str(e) and attempt < retries - 1:\n",
    "                wait_time = min(backoff_factor ** attempt, 30)\n",
    "                print(f\"Rate limit hit. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {attempt + 1} attempts. Error: {e}\")\n",
    "                raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for partial results\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as file:\n",
    "        output = json.load(file)\n",
    "else:\n",
    "    output = {}\n",
    "\n",
    "# Skip already processed rows\n",
    "processed_ids = set(output.keys())\n",
    "remaining_data = [row for row in data if str(row['id']) not in processed_ids]\n",
    "\n",
    "# Batch size and configuration\n",
    "batch_size = 20\n",
    "num_threads = 1\n",
    "\n",
    "# Progress bar setup\n",
    "total_batches = (len(remaining_data) + batch_size - 1) // batch_size\n",
    "progress = tqdm(total=total_batches, desc=\"Processing Batches\", unit=\"batch\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(remaining_data), batch_size):\n",
    "    batch = remaining_data[i:i + batch_size]\n",
    "    try:\n",
    "        batch_output = process_batch_with_retry(batch)\n",
    "        output.update(batch_output)\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Truncated response detected: {ve}\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    progress.update(1)\n",
    "\n",
    "progress.close()\n",
    "end_time = time.time()\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Experiment completed. Scores saved to '{output_file}'.\")\n",
    "print(f\"Total time taken: {(end_time - start_time) / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session Prompt Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import re  # Import regex for JSON cleaning\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "data_path = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/Final Files/filtered_df_1.json\"\n",
    "output_file = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/Results/Round 5/llama_3.2_3B_1.json\"\n",
    "\n",
    "# Load the dataset from JSON file\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create the full prompt\n",
    "def create_prompt(system_message, user_message):\n",
    "    return f\"{system_message}\\n\\n{user_message}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SESSION PROMPT (DNA+CoT+Personalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "system_message = \"\"\"You are a search quality rater evaluating the usefulness of clicked documents on a web page clicked by users in a session. You have been given the following session information:\n",
    "  a.  User: User ID\n",
    "  b.  Session ID: Unique ID for each session.\n",
    "  c.  Query ID: Unique ID for each query.\n",
    "  d.  Query: User submitted query.\n",
    "  e.  ID: Unique ID for each row.\n",
    "  f.  Clicked Documents: User clicked URL.\n",
    "  g.  Title: Title of the clicked documents.\n",
    "  h.  Content: Brief description of the clicked documents on the web page.\n",
    "  i.  Rank: Rank of clicked documents on the SERP labeled as 1(high)~10(low).\n",
    "  j.  Task Satisfaction: User‚Äôs satisfaction on overall task labeled as 0(low)~4(high).\n",
    "  k.  Query Satisfaction: User‚Äôs satisfaction on a search query labeled as 0(low)~4(high).\n",
    "  l.  CTR: The percentage of clicks a document (URL) receives out of total interactions within a specific task and query.\n",
    "  m.  Session Dwell Time (ms): Time spent on a specific task across all queries and clicked documents.\n",
    "  n.  Query Dwell Time (ms): Time spent on a specific query.\n",
    "\n",
    "\n",
    "You must analyze each user-session by considering all the information given above and for each clicked document (URL), provide a usefulness score on an integer scale of 0 to 3 with the following meanings:\n",
    "3 = Very Useful, very helpful for this query\n",
    "2 = Fairly Useful, fairly helpful for this query\n",
    "1 = Somewhat Useful, maybe partly helpful but might contain other irrelevant content\n",
    "0 = Not Useful at all, should never be shown for this query\n",
    "\n",
    "Important Instructions:\n",
    "Consider all the attributes above while deciding on a usefulness score. If certain attributes are unavailable, rely on the available ones to decide. Assign category 1 if the\n",
    "clicked document is somewhat useful to the task and query but not completely, category 2 if the clicked document presents something very important related to the task and\n",
    "query but also has some extra information, and category 3 if the clicked document only and entirely refers to the task and document. If none of the above satisfies give it category 0.\n",
    "\n",
    "You will be provided data in the form of:\n",
    "[\n",
    "    {\n",
    "        \"user_id\": \"<USER>\",\n",
    "        \"session_id\": \"<SESSION_ID>\",\n",
    "        \"session_dwell_time\": \"<SESSION_DWELL_TIME>\",\n",
    "        \"session_sat_score\": \"<SESSION_SAT_SCORE>\",\n",
    "        \"queries\": [\n",
    "          {\n",
    "            \"query_id\": \"<QUERY_ID>\",\n",
    "            \"query_string_en\": \"<QUERY_TEXT>\",\n",
    "            \"query_dwell_time\": \"<QUERY_DWELL_TIME>\",\n",
    "            \"query_sat_score\": \"<QUERY_SAT_SCORE>\",\n",
    "            \"urls\": [\n",
    "              {\n",
    "                \"id\": \"<ID>\",\n",
    "                \"url\": \"<URL>\",\n",
    "                \"title_en\": \"<TITLE>\",\n",
    "                \"content_en\": \"<CONTENT>\",\n",
    "                \"rank\": \"<RANK>\",\n",
    "                \"CTR\": \"<CLICK_THROUGH_RATE>\",\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "] \n",
    "\n",
    "For each user <USER> and each task session <TASK_ID>, for each clicked document <URL>, split this problem into steps:\n",
    "a. Consider ALL the attributes and relative importance of each and decide on a final score <usefulness_i>. Final score must be an integer value only.\n",
    "b. Prioritise ALL user metrics like CTR, query dwell time and session dwell time as indicators of usefulness.\n",
    "c. Consider user's intent of the query and session ensuring they align with the title and content.\n",
    "d. Consider the rank of URLs relation to the user‚Äôs query.\n",
    "e. Consider user‚Äôs query satisfaction score and session satisfaction score, which reflect their satisfaction with the result.\n",
    "\n",
    "\n",
    "ONLY PROVIDE OUTPUT IN GIVEN FORMAT. Directly output the usefulness score for unique ID <ID> as an integer value in the following JSON format:\n",
    "{\\\"<id_1>\\\": <usefulness_1>, ..., \\\"<id_n>\\\": <usefulness_n>}\n",
    "\n",
    "GENERATE USEFULNESS SCORE IN CORRECT OUTPUT FORMAT FOR ALL UNIQUE ID. DO NOT PROVIDE ADDITIONAL TEXT, REASONING, EXAMPLE, OR CODE. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SESSION PROMPT ROUND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_task_input(task_session):\n",
    "    \"\"\"\n",
    "    Generate a structured task input string for LLaMA 3.3 70B based on the provided JSON structure.\n",
    "    \"\"\"\n",
    "    task_context = f\"User: {task_session.get('user', 'Unknown')}\\nTasks:\\n\"\n",
    "\n",
    "    for task in task_session.get('tasks', []):\n",
    "        task_context += (\n",
    "            f\"  - Task ID: {task.get('task_id', 'Unknown')}\\n\"\n",
    "            f\"    Task Description: {task.get('task', 'Unknown')}\\n\"\n",
    "            f\"    Task Dwell Time: {task.get('task_dwell_time', 'Unknown')}\\n\"\n",
    "            f\"    Task Satisfaction Score: {task.get('task_sat_score', 'Unknown')}\\n\"\n",
    "            f\"    Queries:\\n\"\n",
    "        )\n",
    "\n",
    "        for query in task.get('queries', []):\n",
    "            task_context += (\n",
    "                f\"      - Query: {query.get('trs_query', 'Unknown')}\\n\"\n",
    "                f\"        Query Position: {query.get('query_position', 'Unknown')}\\n\"\n",
    "                f\"        Query Dwell Time: {query.get('query_dwell_time', 'Unknown')}\\n\"\n",
    "                f\"        Query Satisfaction Score: {query.get('query_sat_score', 'Unknown')}\\n\"\n",
    "                f\"        Clicked URLs:\\n\"\n",
    "            )\n",
    "\n",
    "            for url in query.get('urls', []):\n",
    "                task_context += (\n",
    "                    f\"          - ID: {url.get('id', 'Unknown')}\\n\"\n",
    "                    f\"            URL: {url.get('url', 'Unknown')}\\n\"\n",
    "                    f\"            Title: {url.get('title_en', 'Unknown')}\\n\"\n",
    "                    f\"            Summary: {url.get('summary_en', 'Unknown')}\\n\"\n",
    "                    f\"            Rank: {url.get('rank', 'Unknown')}\\n\"\n",
    "                    f\"            Task Relevance: {url.get('task_relevance', 'Unknown')}\\n\"\n",
    "                    f\"            Query Relevance: {url.get('query_relevance', 'Unknown')}\\n\"\n",
    "                    f\"            URL Dwell Time: {url.get('url_dwell_time', 'Unknown')}\\n\"\n",
    "                    f\"            Click-Through Rate (CTR): {url.get('CTR', 'Unknown')}\\n\"\n",
    "                \n",
    "                )\n",
    "\n",
    "    return task_context.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix JSON formatting before parsing\n",
    "def fix_json_format(response_content):\n",
    "    \"\"\"\n",
    "    Cleans up LLM output to remove markdown formatting and ensures valid JSON structure.\n",
    "    \"\"\"\n",
    "    response_content = response_content.strip()\n",
    "\n",
    "   #Remove ALL leading/trailing triple backticks (```json ... ```)\n",
    "    response_content = re.sub(r\"^```[a-zA-Z]*\", \"\", response_content).strip()  # Remove leading ```json or ```\n",
    "    response_content = re.sub(r\"```$\", \"\", response_content).strip()  # Remove trailing ```\n",
    "\n",
    "    # Fix missing commas between JSON objects\n",
    "    response_content = re.sub(r\"}\\s*{\", \"}, {\", response_content)\n",
    "\n",
    "    #Ensure JSON starts and ends correctly\n",
    "    if not response_content.startswith(\"{\"):\n",
    "        response_content = \"{\" + response_content.lstrip('{')\n",
    "    if not response_content.endswith(\"}\"):\n",
    "        response_content = response_content.rstrip('}') + \"}\"\n",
    "\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_task_with_retry(task, llm, system_message, retries=3, backoff_factor=2, max_wait=30):\n",
    "    \"\"\"\n",
    "    Processes a single task with retry logic, handling truncation and rate limits.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            task_input = create_task_input(task)\n",
    "            full_prompt = create_prompt(system_message, task_input)\n",
    "\n",
    "            # Invoke the LLM\n",
    "            response = llm.invoke(full_prompt)\n",
    "\n",
    "            # Extract the text response safely\n",
    "            response_content = getattr(response, \"content\", \"\").strip()\n",
    "            if not response_content:\n",
    "                print(f\"Warning: Empty LLM response for Task ID {task.get('task_id', 'Unknown')}\")\n",
    "                continue\n",
    "\n",
    "            # Log raw response for debugging\n",
    "            print(f\"\\nüîç Raw LLM Response for Task ID {task.get('task_id', 'Unknown')}:\\n{response_content}\\n\")\n",
    "\n",
    "            # Fix JSON formatting issues\n",
    "            response_content = fix_json_format(response_content)\n",
    "\n",
    "            # Attempt JSON parsing\n",
    "            try:\n",
    "                parsed_response = json.loads(response_content)\n",
    "\n",
    "                # Ensure response is a dictionary `{ \"id\": usefulness }`\n",
    "                if not isinstance(parsed_response, dict):\n",
    "                    print(f\"Warning: Unexpected JSON structure from LLM for Task ID {task.get('task_id', 'Unknown')}\")\n",
    "                    continue  # Retry\n",
    "\n",
    "                return parsed_response \n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError for Task ID {task.get('task_id', 'Unknown')}: {e}\")\n",
    "                print(\"Skipping task due to JSON issues.\")\n",
    "                return None  # Skip this task instead of retrying\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"Rate limit\" in str(e) and attempt < retries - 1:\n",
    "                wait_time = min(backoff_factor ** attempt, max_wait)\n",
    "                print(f\"‚è≥ Rate limit hit. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {attempt + 1} attempts for Task ID {task.get('task_id', 'Unknown')}. Error: {e}\")\n",
    "                raise e\n",
    "\n",
    "    print(f\"Task ID {task.get('task_id', 'Unknown')} failed after {retries} attempts.\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load existing `{ \"id\": usefulness }` dictionary\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as file:\n",
    "        output = json.load(file)\n",
    "else:\n",
    "    output = {} \n",
    "\n",
    "# Track already processed IDs\n",
    "processed_ids = set(output.keys())\n",
    "\n",
    "# Extract remaining tasks\n",
    "remaining_tasks = []\n",
    "for user_data in data:\n",
    "    for task in user_data.get(\"tasks\", []):\n",
    "        all_urls = [url[\"id\"] for query in task.get(\"queries\", []) for url in query.get(\"urls\", [])]\n",
    "        unprocessed_urls = [url for url in all_urls if url not in processed_ids]\n",
    "\n",
    "        if unprocessed_urls:  \n",
    "            remaining_tasks.append({\"tasks\": [task]})\n",
    "\n",
    "# Bar setup\n",
    "progress = tqdm(total=len(remaining_tasks), desc=\"Processing Tasks\", unit=\"task\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each task\n",
    "for task_session in remaining_tasks:\n",
    "    try:\n",
    "        task_id = task_session[\"tasks\"][0][\"task_id\"]\n",
    "        task_output = process_task_with_retry(task_session, llm, system_message)\n",
    "\n",
    "        if task_output is None:\n",
    "            print(f\"Skipping Task {task_id} due to LLM failure.\")\n",
    "            progress.update(1) \n",
    "            continue\n",
    "\n",
    "        for task in task_session[\"tasks\"]:\n",
    "            for query in task.get(\"queries\", []):\n",
    "                for url_data in query.get(\"urls\", []):\n",
    "                    url_id = url_data.get(\"id\")\n",
    "\n",
    "                    if not url_id or url_id in processed_ids:\n",
    "                        continue\n",
    "\n",
    "                    usefulness_score = task_output.get(str(url_id))\n",
    "\n",
    "                    if usefulness_score is not None:\n",
    "                        output[str(url_id)] = usefulness_score  \n",
    "                        processed_ids.add(url_id)\n",
    "\n",
    "\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Truncated response detected for Task {task_id}: {ve}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Task {task_id}: {e}\")\n",
    "\n",
    "   \n",
    "    progress.update(1)  \n",
    "\n",
    "progress.close()\n",
    "end_time = time.time()\n",
    "\n",
    "#Final save\n",
    "if output:\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Experiment completed. Scores saved to '{output_file}'.\")\n",
    "print(f\"Total time taken: {(end_time - start_time) / 60:.2f} minutes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
