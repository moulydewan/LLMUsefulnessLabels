{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installing and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-aws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tqdm\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AWS API Keys\n",
    "with open(\"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/secrets.yml\", 'r') as file:\n",
    "    credentials = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing LLMs (Llama via Bedrock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatBedrock with LLaMA 3.3 70B\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.meta.llama3-3-70b-instruct-v1:0\",\n",
    "    region=\"us-east-1\",\n",
    "    aws_access_key_id=credentials[\"bedrock\"][\"access_key\"],\n",
    "    aws_secret_access_key=credentials[\"bedrock\"][\"secret_key\"],\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatBedrock with LLaMA 3.2 3B\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.meta.llama3-2-3b-instruct-v1:0\",\n",
    "    region=\"us-east-1\",\n",
    "    aws_access_key_id=credentials[\"bedrock\"][\"access_key\"],\n",
    "    aws_secret_access_key=credentials[\"bedrock\"][\"secret_key\"],\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatBedrock with LLaMA 3.1 8B\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"us.meta.llama3-1-8b-instruct-v1:0\",\n",
    "    region=\"us-east-1\",\n",
    "    aws_access_key_id=credentials[\"bedrock\"][\"access_key\"],\n",
    "    aws_secret_access_key=credentials[\"bedrock\"][\"secret_key\"],\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0,\n",
    "        \"top_p\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Prompt Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/Final Files/Rubrics Reasoning/QREF/qref_final_reasoning.json\"\n",
    "output_file = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/Final Files/Rubrics Reasoning/QREF/llama_3_3_rubrics_3.json\"\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "def create_prompt(system_message, user_message):\n",
    "    return f\"{system_message}\\n\\n{user_message}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BASELINE PROMPT (DNA+CoT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a search quality rater evaluating the usefulness of clicked documents on a web page clicked by users in a session. You have been given the following session information:\n",
    "  a.  User: User ID\n",
    "  b.  Task ID: Unique ID for each task.\n",
    "  c.  Task Description: A clear explanation of what the user tries to accomplish using the search engine.\n",
    "  d.  Query: User submitted query.\n",
    "  e.  ID: Unique ID for each row.\n",
    "  f.  Clicked Documents: User clicked URL.\n",
    "  g.  Title: Title of the clicked documents.\n",
    "  h.  Summary: Brief description of the clicked documents on the web page.\n",
    "  i.  Rank: Rank of clicked documents on the SERP labeled as 1(high)~30(low).\n",
    "  j.  Task Relevance: Relevance of clicked documents to the userâ€™s task labeled as 0 (low)~3 (high).\n",
    "  k.  Query Relevance: Relevance of clicked documents to the userâ€™s query labeled as 0 (low)~3 (high).\n",
    "  l.  Task Satisfaction: Userâ€™s satisfaction on overall task labeled as 1(low)~5(high).\n",
    "  m.  Query Satisfaction: Userâ€™s satisfaction on a search query labeled as 1(low)~5(high).\n",
    "  n.  CTR: The percentage of clicks a document (URL) receives out of total interactions within a specific task and query.\n",
    "  o.  URL Dwell Time (ms): Time spent on a clicked document.\n",
    "  p.  Task Dwell Time (ms): Time spent on a specific task across all queries and clicked documents.\n",
    "  q.  Query Dwell Time (ms): Time spent on a specific query.\n",
    "\n",
    "\n",
    "You must analyze each task session by considering all the information given above and for each clicked document (URL), provide a usefulness score on an integer scale of 0 to 3 with the following meanings:\n",
    "3 = Very Useful, very helpful for this query\n",
    "2 = Fairly Useful, fairly helpful for this query\n",
    "1 = Somewhat Useful, maybe partly helpful but might contain other irrelevant content\n",
    "0 = Not Useful at all, should never be shown for this query\n",
    "\n",
    "Important Instructions:\n",
    "Consider all the attributes above while deciding on a usefulness score. If certain attributes are unavailable, rely on the available ones to decide. Assign category 1 if the\n",
    "clicked document is somewhat useful to the task and query but not completely, category 2 if the clicked document presents something very important related to the task and\n",
    "query but also has some extra information, and category 3 if the clicked document only and entirely refers to the task and document. If none of the above satisfies give it category 0.\n",
    "\n",
    "You will be provided data in the form of:\n",
    "[\n",
    "  {\n",
    "    \"user\": \"<USER>\",\n",
    "    \"task_id\": \"<TASK_ID>\",\n",
    "    \"task\": \"<TASK_DESCRIPTION>\",\n",
    "    \"task_dwell_time\": \"<TASK_DWELL_TIME>\",\n",
    "    \"task_sat_score\": \"<TASK_SAT_SCORE>\",\n",
    "    \"trs_query\": \"<QUERY_TEXT>\",\n",
    "    \"query_dwell_time\": \"<QUERY_DWELL_TIME>\",\n",
    "    \"query_sat_score\": \"<QUERY_SAT_SCORE>\",\n",
    "    \"id\": \"<ID>\"\n",
    "    \"url\": \"<URL>\",\n",
    "    \"title_en\": \"<TITLE>\",\n",
    "    \"summary_en\": \"<SUMMARY>\",\n",
    "    \"rank\": \"<RANK>\",\n",
    "    \"task_relevance\": \"<TASK_RELEVANCE>\",\n",
    "    \"query_relevance\": \"<QUERY_RELEVANCE>\",\n",
    "    \"url_dwell_time\": \"<URL_DWELL_TIME>\",\n",
    "    \"CTR\": \"<CLICK_THROUGH_RATE>\",\n",
    "  }\n",
    "]\n",
    "\n",
    "For each user <USER> and each task session <TASK_ID>, for each clicked document <URL>, split this problem into steps:\n",
    "a. Consider ALL the attributes and relative importance of each and decide on a final score <usefulness_i>. Final score must be an integer value only.\n",
    "b. Prioritise ALL user metrics like CTR, URL dwell time, query dwell time and session dwell time as indicators of usefulness.\n",
    "c. Consider user's intent of the search and task session ensuring they align with the title and summary.\n",
    "d. Consider the rank of URLs relation to the userâ€™s query and task description.\n",
    "e. Consider task relevance and query relevance which indicate how well the result aligns with the userâ€™s overall goal.\n",
    "f. Consider userâ€™s query satisfaction score and session satisfaction score, which reflect their satisfaction with the result.\n",
    "\n",
    "\n",
    "ONLY PROVIDE OUTPUT IN GIVEN FORMAT. Directly output the usefulness score for unique ID <ID> as an integer value in the following JSON format:\n",
    "{\\\"<id_1>\\\": <usefulness_1>, ..., \\\"<id_n>\\\": <usefulness_n>}\n",
    "\n",
    "DO NOT PROVIDE ADDITIONAL TEXT, REASONING, EXAMPLE, OR CODE. GENERATE USEFULNESS SCORE IN CORRECT OUTPUT FORMAT FOR ALL UNIQUE ID.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create batched input\n",
    "def create_batched_input(batch):\n",
    "    \"\"\"\n",
    "    Combine multiple rows into a single user message for batch processing.\n",
    "    \"\"\"\n",
    "    batched_user_message = \"\\n\\n\".join(\n",
    "        f\"\"\"\n",
    "        id: {row['id']}\n",
    "        user: {row['user']}\n",
    "        task_id: {row['task_id']}\n",
    "        task: {row['task']}\n",
    "        task_sat_score: {row['task_sat_score']}\n",
    "        task_dwell_time: {row['task_dwell_time']}\n",
    "        trs_query: {row['trs_query']}\n",
    "        query_position: {row['query_position']}\n",
    "        query_sat_score: {row['query_sat_score']}\n",
    "        query_dwell_time: {row['query_dwell_time']}\n",
    "        url: {row['url']}\n",
    "        title: {row['title_en']}\n",
    "        summary: {row['summary_en']}\n",
    "        rank: {row['rank']}\n",
    "        task_relevance: {row['task_relevance']}\n",
    "        query_relevance: {row['query_relevance']}\n",
    "        url_dwell_time: {row['url_dwell_time']}\n",
    "        avg_url_dwell_time: {row['avg_url_dwell_time']}\n",
    "        CTR: {row['CTR']}\n",
    "        \"\"\"\n",
    "        for row in batch\n",
    "    )\n",
    "    return batched_user_message.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recover JSON from a truncated response\n",
    "def recover_json(truncated_response):\n",
    "    \"\"\"\n",
    "    Attempt to recover valid JSON from a truncated response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove backticks and extra formatting\n",
    "        truncated_response = truncated_response.strip('```json').strip('```').strip()\n",
    "        \n",
    "        # Attempt to parse progressively shorter substrings of the response\n",
    "        for i in range(len(truncated_response), 0, -1):\n",
    "            try:\n",
    "                return json.loads(truncated_response[:i])  \n",
    "            except json.JSONDecodeError:\n",
    "                continue \n",
    "    except Exception as e:\n",
    "        print(f\"Error recovering JSON: {e}\")\n",
    "    return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch_with_retry(batch, retries=3, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Process a batch of rows by combining them into a single request, with backtick removal and truncated response handling.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Combine rows into a single input\n",
    "            batched_user_message = create_batched_input(batch)\n",
    "            full_prompt = create_prompt(system_message, batched_user_message)\n",
    "\n",
    "            # Send the combined prompt to the LLM\n",
    "            response = llm.predict(full_prompt)\n",
    "            \n",
    "            # Print the log raw response\n",
    "            print(f\"Raw LLM response:\\n{response}\")\n",
    "\n",
    "            # Clean the response by removing backticks\n",
    "            cleaned_response = response.strip('```json').strip('```').strip()\n",
    "\n",
    "            # Check if the response is truncated\n",
    "            if not cleaned_response.endswith(\"}\"):\n",
    "                print(\"Warning: Truncated response detected.\")\n",
    "                print(f\"Truncated response: {cleaned_response}\")  # Log for debugging\n",
    "\n",
    "                # Save truncated responses for manual review\n",
    "                with open(\"truncated_responses.json\", \"a\", encoding=\"utf-8\") as file:\n",
    "                    file.write(cleaned_response + \"\\n\")\n",
    "\n",
    "                # Attempt to recover JSON\n",
    "                recovered_response = recover_json(cleaned_response)\n",
    "                if recovered_response:\n",
    "                    print(\"Recovered partial JSON response successfully.\")\n",
    "                    return recovered_response\n",
    "\n",
    "                # Retry with smaller batch sizes if recovery fails\n",
    "                print(\"Unable to recover truncated response. Retrying with smaller batch size.\")\n",
    "                smaller_batches = [batch[j:j+len(batch)//2] for j in range(0, len(batch), len(batch)//2)]\n",
    "                for small_batch in smaller_batches:\n",
    "                    process_batch_with_retry(small_batch)\n",
    "                return {}\n",
    "\n",
    "            # Parse the cleaned response as JSON\n",
    "            parsed_response = json.loads(cleaned_response)\n",
    "            print(f\"Parsed response: {parsed_response}\")  # Log parsed response for debugging\n",
    "            return parsed_response\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"Rate limit\" in str(e) and attempt < retries - 1:\n",
    "                wait_time = min(backoff_factor ** attempt, 30)\n",
    "                print(f\"Rate limit hit. Retrying in {wait_time} seconds... (Attempt {attempt + 1}/{retries})\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {attempt + 1} attempts. Error: {e}\")\n",
    "                raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for partial results\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as file:\n",
    "        output = json.load(file)  # Load already processed results\n",
    "else:\n",
    "    output = {}\n",
    "\n",
    "# Skip already processed rows\n",
    "processed_ids = set(output.keys())  # Extract processed unique IDs\n",
    "remaining_data = [row for row in data if str(row['id']) not in processed_ids]  # Filter remaining rows\n",
    "\n",
    "# Batch size and processing configuration\n",
    "batch_size = 1  # Adjust as needed\n",
    "num_threads = 1  # Use single-threaded processing to minimize rate limits\n",
    "\n",
    "# Progress bar setup\n",
    "total_batches = (len(remaining_data) + batch_size - 1) // batch_size\n",
    "progress = tqdm(total=total_batches, desc=\"Processing Remaining Batches\", unit=\"batch\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(remaining_data), batch_size):\n",
    "    batch = remaining_data[i:i + batch_size]\n",
    "    try:\n",
    "        # Log batch details\n",
    "        print(f\"Processing batch {i // batch_size + 1}/{total_batches} with {len(batch)} rows.\")\n",
    "        #print(f\"Batched input:\\n{create_batched_input(batch)}\")  # Debug log for input\n",
    "\n",
    "        # Process the batch\n",
    "        batch_output = process_batch_with_retry(batch)\n",
    "        \n",
    "        # Log raw output\n",
    "        print(f\"Batch Output:\\n{batch_output}\")\n",
    "\n",
    "        # Update the results dictionary\n",
    "        output.update(batch_output)\n",
    "\n",
    "        # Save intermediate results\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Intermediate results saved. Total entries: {len(output)}\")\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Truncated response detected in batch {i // batch_size + 1}: {ve}\")\n",
    "        # Save partial results even if the batch fails\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Partial results saved after error. Total entries: {len(output)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch {i // batch_size + 1}: {e}\")\n",
    "        # Save partial results for unexpected errors\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "        print(f\"Partial results saved after unexpected error. Total entries: {len(output)}\")\n",
    "\n",
    "    progress.update(1)\n",
    "\n",
    "progress.close()\n",
    "end_time = time.time()\n",
    "\n",
    "# Final save\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Experiment completed. Scores saved to '{output_file}'.\")\n",
    "print(f\"Total time taken: {(end_time - start_time) / 60:.2f} minutes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Session Prompt Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "data_path = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/Final Files/filtered_df_1.json\"\n",
    "output_file = \"/Users/mdewan/Documents/Research Projects/Usefulness IR'24/KDD19_UserStudy/Results/Round 5/llama_3.2_3B_1.json\"\n",
    "\n",
    "with open(data_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the full prompt\n",
    "def create_prompt(system_message, user_message):\n",
    "    return f\"{system_message}\\n\\n{user_message}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SESSION PROMPT (DNA+CoT+Personalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session prompt template\n",
    "system_message = \"\"\"You are a search quality rater evaluating the usefulness of clicked documents on a web page. You have been given the following information:\n",
    "  a.  User: User ID\n",
    "  b.  Task ID: Unique ID for each task.\n",
    "  c.  Task Description: A clear explanation of what the user tries to accomplish using the search engine.\n",
    "  d.  Query: User submitted query.\n",
    "  e.  ID: Unique ID for each row.\n",
    "  f.  Clicked Documents: User clicked URL.\n",
    "  g.  Title: Title of the clicked documents.\n",
    "  h.  Summary: Brief description of the clicked documents on the web page.\n",
    "  i.  Rank: The rank of clicked documents on the result list.\n",
    "  j.  Task Relevance: Relevance of clicked documents to the userâ€™s task labeled as 0 (low)~3 (high).\n",
    "  k.  Query Relevance: Relevance of clicked documents to the userâ€™s query labeled as 0 (low)~3 (high).\n",
    "  l.  Task Satisfaction: Userâ€™s satisfaction on overall task labeled as 1(low)~5(high).\n",
    "  m.  Query Satisfaction: Userâ€™s satisfaction on a search query labeled as 1(low)~5(high).\n",
    "  n.  URL Click-Through Rate (CTR): The percentage of clicks a document (URL) receives out of total interactions within a specific task and query.\n",
    "  o.  URL Click Dwell Time: Time spent on a clicked document.\n",
    "  p.  Task Dwell Time: Time spent on a specific task across all queries and clicked documents.\n",
    "  q.  Query Dwell Time: Time spent on a specific query.\n",
    "\n",
    "You must analyze each task session by considering all the information given above and for each clicked document (URL), provide a usefulness score on an integer scale of 0 to 3 with the following meanings:\n",
    "3 = Very Useful, very helpful for this query\n",
    "2 = Fairly Useful, fairly helpful for this query\n",
    "1 = Somewhat Useful, maybe partly helpful but might contain other irrelevant content\n",
    "0 = Not Useful at all, should never be shown for this query\n",
    "\n",
    "Important Instructions:\n",
    "Consider all the attributes above while deciding on a usefulness score. If certain attributes are unavailable, rely on the available ones to decide. Assign category 1 if the\n",
    "clicked document is somewhat useful to the task and query but not completely, category 2 if the clicked document presents something very important related to the task and\n",
    "query but also has some extra information, and category 3 if the clicked document only and entirely refers to the task and document. If none of the above satisfies give it category 0.\n",
    "\n",
    "You will be provided data in the form of:\n",
    "\n",
    "{\n",
    "  \"instruction\": \"Analyze each task session <TASK_ID> by each user <USER> and provide usefulness labels for each clicked document <URL>.\",\n",
    "  \"data\": {\n",
    "    \"user\": \"<USER>\",\n",
    "    \"tasks\": [\n",
    "      {\n",
    "        \"task_id\": \"<TASK_ID>\",\n",
    "        \"task\": \"<TASK_DESCRIPTION>\",\n",
    "        \"task_dwell_time\": \"<TASK_DWELL_TIME>\",\n",
    "        \"task_sat_score\": \"<TASK_SAT_SCORE>\",\n",
    "        \"queries\": [\n",
    "          {\n",
    "            \"trs_query\": \"<QUERY_TEXT>\",\n",
    "            \"query_dwell_time\": \"<QUERY_DWELL_TIME>\",\n",
    "            \"query_sat_score\": \"<QUERY_SAT_SCORE>\",\n",
    "            \"urls\": [\n",
    "              {\n",
    "                \"id\": \"<ID>\",\n",
    "                \"url\": \"<URL>\",\n",
    "                \"title_en\": \"<TITLE>\",\n",
    "                \"summary_en\": \"<SUMMARY>\",\n",
    "                \"rank\": \"<RANK>\",\n",
    "                \"task_relevance\": \"<TASK_RELEVANCE>\",\n",
    "                \"query_relevance\": \"<QUERY_RELEVANCE>\",\n",
    "                \"CTR\": \"<CLICK_THROUGH_RATE>\",\n",
    "                \"url_dwell_time\": \"<URL_DWELL_TIME>\"\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "For each user <USER> and each task session <TASK_ID>, for each clicked document <URL>, split this problem into steps:\n",
    "a. Consider the underlying intent of the search and task session.\n",
    "b. Consider the title and summary ensuring they align with the userâ€™s intent.\n",
    "c. Consider the rank of the result in relation to the userâ€™s query and task description.\n",
    "d. Consider task relevance and query relevance which indicate how well the result aligns with the userâ€™s overall goal.\n",
    "e. Consider userâ€™s query satisfaction score and session satisfaction score, which reflect their satisfaction with the result.\n",
    "f. Consider user search behavior metrics like CTR, URL click dwell time, query dwell time, and session dwell time as indicators of usefulness.\n",
    "g. Consider all the attributes above and relative importance of each and decide on a final score <usefulness_i>. Final score must be an integer value only.\n",
    "\n",
    "Directly output the usefulness score for each unique row <ID> as an integer value in the following JSON format:\n",
    "{\\\"<id_1>\\\": <usefulness_1>, ..., \\\"<id_n>\\\": <usefulness_n>}.\n",
    "DO NOT PROVIDE ADDITIONAL REASONING< EXAMPLES OR CODE. Just provide the output.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get the session input\n",
    "def create_task_input(task_session):\n",
    "    \"\"\"\n",
    "    Generate a structured task input string for LLaMA 3.3 70B based on the provided JSON structure.\n",
    "    \"\"\"\n",
    "    task_context = f\"User: {task_session.get('user', 'Unknown')}\\nTasks:\\n\"\n",
    "\n",
    "    for task in task_session.get('tasks', []):\n",
    "        task_context += (\n",
    "            f\"  - Task ID: {task.get('task_id', 'Unknown')}\\n\"\n",
    "            f\"    Task Description: {task.get('task', 'Unknown')}\\n\"\n",
    "            f\"    Task Dwell Time: {task.get('task_dwell_time', 'Unknown')}\\n\"\n",
    "            f\"    Task Satisfaction Score: {task.get('task_sat_score', 'Unknown')}\\n\"\n",
    "            f\"    Queries:\\n\"\n",
    "        )\n",
    "\n",
    "        for query in task.get('queries', []):\n",
    "            task_context += (\n",
    "                f\"      - Query: {query.get('trs_query', 'Unknown')}\\n\"\n",
    "                f\"        Query Position: {query.get('query_position', 'Unknown')}\\n\"\n",
    "                f\"        Query Dwell Time: {query.get('query_dwell_time', 'Unknown')}\\n\"\n",
    "                f\"        Query Satisfaction Score: {query.get('query_sat_score', 'Unknown')}\\n\"\n",
    "                f\"        Clicked URLs:\\n\"\n",
    "            )\n",
    "\n",
    "            for url in query.get('urls', []):\n",
    "                task_context += (\n",
    "                    f\"          - ID: {url.get('id', 'Unknown')}\\n\"\n",
    "                    f\"            URL: {url.get('url', 'Unknown')}\\n\"\n",
    "                    f\"            Title: {url.get('title_en', 'Unknown')}\\n\"\n",
    "                    f\"            Summary: {url.get('summary_en', 'Unknown')}\\n\"\n",
    "                    f\"            Rank: {url.get('rank', 'Unknown')}\\n\"\n",
    "                    f\"            Task Relevance: {url.get('task_relevance', 'Unknown')}\\n\"\n",
    "                    f\"            Query Relevance: {url.get('query_relevance', 'Unknown')}\\n\"\n",
    "                    f\"            URL Dwell Time: {url.get('url_dwell_time', 'Unknown')}\\n\"\n",
    "                    f\"            Avg URL Dwell Time Per User Per Task: {url.get('avg_url_dwell_time_per_user_task', 'Unknown')}\\n\"\n",
    "                    f\"            Avg URL Dwell Time Per User: {url.get('avg_url_dwell_time_per_user', 'Unknown')}\\n\"\n",
    "                    f\"            Click-Through Rate (CTR): {url.get('CTR', 'Unknown')}\\n\"\n",
    "                    f\"            Total Queries Per User Per Task: {url.get('total_queries', 'Unknown')}\\n\"\n",
    "                    f\"            Max Clicks Per User Per Query: {url.get('max_clicks_per_user_query', 'Unknown')}\\n\"\n",
    "                )\n",
    "\n",
    "    return task_context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fix JSON formatting before parsing\n",
    "def fix_json_format(response_content):\n",
    "    \"\"\"\n",
    "    Cleans up LLM output to remove markdown formatting and ensures valid JSON structure.\n",
    "    \"\"\"\n",
    "    response_content = response_content.strip()\n",
    "\n",
    "    # Remove ALL leading/trailing triple backticks (```json ... ```)\n",
    "    response_content = re.sub(r\"^```[a-zA-Z]*\", \"\", response_content).strip()  # Remove leading ```json or ```\n",
    "    response_content = re.sub(r\"```$\", \"\", response_content).strip()  # Remove trailing ```\n",
    "\n",
    "    # Fix missing commas between JSON objects\n",
    "    response_content = re.sub(r\"}\\s*{\", \"}, {\", response_content)\n",
    "\n",
    "    # Ensure JSON starts and ends correctly\n",
    "    if not response_content.startswith(\"{\"):\n",
    "        response_content = \"{\" + response_content.lstrip('{')\n",
    "    if not response_content.endswith(\"}\"):\n",
    "        response_content = response_content.rstrip('}') + \"}\"\n",
    "\n",
    "    return response_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function with missed entry checks\n",
    "def process_task_with_retry(task, llm, system_message, retries=3, backoff_factor=2, max_wait=30):\n",
    "    \"\"\"\n",
    "    Processes a single task with retry logic, handling truncation and rate limits.\n",
    "    \"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            task_input = create_task_input(task)\n",
    "            full_prompt = create_prompt(system_message, task_input)\n",
    "\n",
    "            # Invoke the LLM\n",
    "            response = llm.invoke(full_prompt)\n",
    "\n",
    "            # Extract the text responses\n",
    "            response_content = getattr(response, \"content\", \"\").strip()\n",
    "            if not response_content:\n",
    "                print(f\"Warning: Empty LLM response for Task ID {task.get('task_id', 'Unknown')}\")\n",
    "                continue  # Retry\n",
    "\n",
    "            # Print raw response in log for debugging\n",
    "            print(f\"\\nðŸ” Raw LLM Response for Task ID {task.get('task_id', 'Unknown')}:\\n{response_content}\\n\")\n",
    "\n",
    "            # Fix JSON formatting issues\n",
    "            response_content = fix_json_format(response_content)\n",
    "\n",
    "            # Attempt JSON parsing\n",
    "            try:\n",
    "                parsed_response = json.loads(response_content)\n",
    "\n",
    "                # Ensure response is a dictionary `{ \"id\": usefulness }`\n",
    "                if not isinstance(parsed_response, dict):\n",
    "                    print(f\"Warning: Unexpected JSON structure from LLM for Task ID {task.get('task_id', 'Unknown')}\")\n",
    "                    continue  # Retry\n",
    "\n",
    "                # Filter IDs between 1 and 3050 and usefulness scores between 0 and 3\n",
    "                filtered_response = {\n",
    "                    str(url_id): usefulness\n",
    "                    for url_id, usefulness in parsed_response.items()\n",
    "                    if url_id.isdigit() and 1 <= int(url_id) <= 3050 and 0 <= usefulness <= 3\n",
    "                }\n",
    "\n",
    "                return filtered_response \n",
    "\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSONDecodeError for Task ID {task.get('task_id', 'Unknown')}: {e}\")\n",
    "                print(\"Skipping task due to JSON issues.\")\n",
    "                return None  # Skip this task instead of retrying\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"Rate limit\" in str(e) and attempt < retries - 1:\n",
    "                wait_time = min(backoff_factor ** attempt, max_wait)\n",
    "                print(f\"Rate limit hit. Retrying in {wait_time} seconds...\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                print(f\"Failed after {attempt + 1} attempts for Task ID {task.get('task_id', 'Unknown')}. Error: {e}\")\n",
    "                raise e\n",
    "\n",
    "    print(f\"Task ID {task.get('task_id', 'Unknown')} failed after {retries} attempts.\")\n",
    "    return None\n",
    "\n",
    "# Load existing `{ \"id\": usefulness }` dictionary\n",
    "if os.path.exists(output_file):\n",
    "    with open(output_file, 'r', encoding='utf-8') as file:\n",
    "        output = json.load(file)\n",
    "else:\n",
    "    output = {}  \n",
    "\n",
    "#Track already processed IDs\n",
    "processed_ids = set(output.keys())\n",
    "\n",
    "# Find missing IDs between 1 and 3050\n",
    "expected_ids = set(str(i) for i in range(1, 3051))  # Ensure string format matches stored IDs\n",
    "missing_ids = expected_ids - processed_ids  # IDs that need reprocessing\n",
    "\n",
    "if missing_ids:\n",
    "    print(f\"âš ï¸ {len(missing_ids)} IDs are missing. They will be reprocessed.\")\n",
    "\n",
    "# Extract remaining tasks (only for missing IDs)\n",
    "remaining_tasks = []\n",
    "for user_data in data:\n",
    "    for task in user_data.get(\"tasks\", []):\n",
    "        for query in task.get(\"queries\", []):\n",
    "            for url_data in query.get(\"urls\", []):\n",
    "                url_id = str(url_data.get(\"id\"))  # Ensure string format\n",
    "                \n",
    "                if url_id in missing_ids:  #Add only if it's missing\n",
    "                    remaining_tasks.append({\"tasks\": [task]})\n",
    "                    break  #Prevent adding the same task multiple times\n",
    "\n",
    "# Progress bar setup\n",
    "progress = tqdm(total=len(remaining_tasks), desc=\"Processing Tasks\", unit=\"task\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Process each task\n",
    "for task_session in remaining_tasks:\n",
    "    try:\n",
    "        task_id = task_session[\"tasks\"][0][\"task_id\"]\n",
    "        task_output = process_task_with_retry(task_session, llm, system_message)\n",
    "\n",
    "        if task_output is None:\n",
    "            print(f\"Skipping Task {task_id} due to LLM failure.\")\n",
    "            progress.update(1)  \n",
    "            continue\n",
    "\n",
    "        for task in task_session[\"tasks\"]:\n",
    "            for query in task.get(\"queries\", []):\n",
    "                for url_data in query.get(\"urls\", []):\n",
    "                    url_id = str(url_data.get(\"id\")) \n",
    "\n",
    "                    if not url_id or url_id in processed_ids:\n",
    "                        continue\n",
    "\n",
    "                    # Retrieve usefulness score\n",
    "                    usefulness_score = task_output.get(url_id)\n",
    "\n",
    "                    if usefulness_score is not None:\n",
    "                        output[url_id] = usefulness_score  #Store in `{ \"id\": usefulness }` format\n",
    "                        processed_ids.add(url_id)\n",
    "\n",
    "        # Save progress\n",
    "        with open(output_file, 'w', encoding='utf-8') as file:\n",
    "            json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Truncated response detected for Task {task_id}: {ve}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing Task {task_id}: {e}\")\n",
    "\n",
    "    progress.update(1)  \n",
    "\n",
    "progress.close()\n",
    "end_time = time.time()\n",
    "\n",
    "# Final save\n",
    "if output:\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(output, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Experiment completed. Scores saved to '{output_file}'.\")\n",
    "print(f\"Total time taken: {(end_time - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
